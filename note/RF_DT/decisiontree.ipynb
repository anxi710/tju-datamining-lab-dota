{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree - 决策树模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()   # 导入鸢尾花数据集\n",
    "\n",
    "X = iris.data[:, 2:] # petal length and width - 花瓣长度和宽度\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "            tree_clf,\n",
    "            out_file=\"./images/iris_tree.dot\",\n",
    "            feature_names=iris.feature_names[2:],\n",
    "            class_names=iris.target_names,\n",
    "            rounded=True,\n",
    "            filled=True\n",
    ")\n",
    "\n",
    "!dot -Tpng ./images/iris_tree.dot -o ./images/iris_tree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 `$ dot -Tpng iris_tree.dot -o iris_tree.png` 可将 `.dot` 文件转换为 png 格式\n",
    "\n",
    "结果如下（先运行上面的单元格，生成 iris_tree.png）：\n",
    "\n",
    "<img src=\"./images/iris_tree.png\" width=450>\n",
    "\n",
    "节点的 samples 属性统计该节点用于多少个训练样本；value 属性表示该节点对于每个类别有多少个样本；gini 属性用于衡量数据的不纯度。\n",
    "\n",
    "> 数据的不纯度越低，同质性越高。\n",
    ">\n",
    "> 如果一个节点包含的所有训练样本都是同一类的，则 gini = 0\n",
    "\n",
    "Gini Impurity:\n",
    "\n",
    "$$G_i = 1 - \\sum_{k = 1}^{n}{P_{i, k}^2}$$\n",
    "\n",
    "其中 $P_{i, k}$ 表示第 i 个节点中，属于第 k 类的样本所占的比例。\n",
    "\n",
    "> Scikit-Learn 库使用的是 CART 算法，该算法只产生二叉树。然而，ID3 等算法可以产生超过两个字节点的决策树模型\n",
    "\n",
    "---\n",
    "\n",
    "#### 白盒与黑盒：\n",
    "\n",
    "正如我们看到的一样，决策树非常直观，他们的决定很容易被解释。这种模型通常被称为白盒模型。\n",
    "\n",
    "相反，随机森林或神经网络通常被认为是黑盒模型。他们能做出很好的预测，并且可以轻松检查它们做出这些预测过程中计算的执行过程。\n",
    "\n",
    "然而，人们通常很难用简单的术语来解释为什么模型会做出这样的预测。例如，如果一个神经网络说一个特定的人出现在图片上，我们很难知道究竟是什么导致了这一个预测的出现：模型是否认出了那个人的眼睛？她的嘴？她的鼻子？她的鞋？或者是否坐在沙发上？\n",
    "\n",
    "相反，决策树提供良好的、简单的分类规则，甚至可以根据需要手动操作（例如鸢尾花分类）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 决策树还可以估计某个样本属于特定类别的概率\n",
    "print(tree_clf.predict_proba([[5, 1.5]]))\n",
    "\n",
    "print(tree_clf.predict([[5, 1.5]]))  # 输出概率最大的类别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CART 训练算法\n",
    "\n",
    "Scikit-Learn 使用分类回归树（Classification And Regression Tree）算法训练决策树。该算法的思想如下：\n",
    "\n",
    "首先寻找使得划分后的子集 Gini 系数最小的特征-阈值对 $(k, t_k)$，使用该特征 $k$ 和阈值 $t_k$ 划分训练集。\n",
    "\n",
    "这一过程通过最小化如下 cost function 实现：\n",
    "\n",
    "$$J(k, t_k) = \\frac{m_{\\text{left}}}{m}G_{\\text{left}} + \\frac{m_{\\text{right}}}{m}G_{\\text{right}}$$\n",
    "\n",
    "其中，$m$ 是样本个数，$G$ 为 Gini 系数。\n",
    "\n",
    "算法成功将训练集划分为两部分之后，会递归地对子集进行划分。当到达预定的最大深度（由 `max_depth` 超参数决定）或找不到可以继续降低 Gini 系数的划分方法后停止分裂。\n",
    "\n",
    "> 除了 `max_depth` 之外，还有一些超参数可以控制决策树的增长条件，如：`min_samples_split`, `min_samples_leaf`, `min_weight_fraction_leaf`, `max_leaf_nodes` 等。\n",
    "\n",
    "CART 算法是一种贪心算法，其贪心地搜索最优划分方式，然后在每一层重复该过程。它不检查是否能够在多层中的所有划分组合中找到最优方案。\n",
    "\n",
    "贪心算法通常会产生一个局部最优解，但不保证是全局最优。\n",
    "\n",
    "找到最优的决策树是一个 NP 完全问题。因此，我们需要设计一个 “合理的”（而不是最佳的）解决方案。\n",
    "\n",
    "#### 计算复杂度\n",
    "\n",
    "在建立好决策树模型后，做出预测需要遍历决策树。决策树通常近似为一个平衡树，因此遍历决策树需要访问 $O(\\log_{2}{m})$ 个节点。\n",
    "\n",
    "由于每个节点只需要检查一个特征的值，因此总体的预测复杂度仅为 $O(\\log_{2}{m})$，与特征数量无关。所以在处理大型训练集时，预测速度也非常快。\n",
    "\n",
    "然而，训练算法时需要比较所有特征（如果设置了 `max_features` 会少一些）。因此，在每个节点的训练复杂度为 $O(nm\\log_{2}{m})$。\n",
    "\n",
    "对于小型训练集（样本少于几千行），Scikit-learn 可以通过设置 `presort=True` 来加速训练。但是这对于较大的训练集来说会显著减慢训练速度。\n",
    "\n",
    "#### 纯度指标\n",
    "\n",
    "##### 1. 信息增益（Information Gain）\n",
    "\n",
    "信息熵（Information Entropy）一般用于衡量随机变量的不确定性，其定义式如下：\n",
    "\n",
    "$$\\operatorname{Ent}(D) = - \\sum_{i = 1}^{N}{p_i \\log_{2}{p_i}}$$\n",
    "\n",
    "其中 $D$ 表示样本集， $N$ 表示样本集分类数，$p_i$ 表示第 $i$ 类样本在样本集所占比例。\n",
    "\n",
    "> 随机变量的不确定性越大，则纯度越低；即信息熵越大，纯度越低。\n",
    "\n",
    "对于一个离散变量而言，可以根据其在样本集中的所有可能取值将原样本集划分为子样本集。\n",
    "\n",
    "衡量一个划分的好坏的标准是划分前后信息熵的减少量，如果信息熵减少得越多，则不确定性下降得越多。\n",
    "\n",
    "这种划分的度量指标称为信息增益（Information Gain），其定义式如下：\n",
    "\n",
    "$$\\operatorname{Gain}(D, k) = \\operatorname{Ent}(D) - \\sum_{j = 1}^{M} \\frac{\\left| D_j \\right|}{|D|} \\operatorname{Ent}\\left( D_j \\right)$$\n",
    "\n",
    "其中 $D$ 表示样本集，$k$ 表示离散特征，$M$ 表示离散特征 $k$ 所有可能取值的数量，$D_j$ 表示样本集中第 $j$ 种取值的子样本集。\n",
    "\n",
    "> 一般而言，并不会将样本集按照特征的所有可能取值进行划分，因为这种划分对于小样本集而言，可能导致划分之后的子集过小。\n",
    ">\n",
    "> 其次，这种划分还可能导致子集数量过多，且对于不同的特征，划分的子集数量不定，生成的决策树在维护上较为复杂。\n",
    ">\n",
    "> 一般而言，决策树在中间节点的测试是对单一特征的单一阈值进行的，因此会在所有可能取值中，找到使得划分前后信息增益最大的特征的阈值。\n",
    "\n",
    "当特征是连续变量时，其取值不像离散变量那样是有限的。这时可以将连续变量在样本集中的所有可能取值排序后俩俩取平均值作为划分点，改写上式。\n",
    "\n",
    "当只检查一个特征的一个阈值时，定义式如下：\n",
    "\n",
    "$$\\begin{align*}\n",
    "T_k &= \\left\\{ \\frac{k_i + k_{i + 1}}{2} | 1 \\le i \\le M - 1 \\right\\} \\\\\n",
    "\\operatorname{Gain}(D, k) &= \\max_{t \\in T_k} \\operatorname{Gain}(D, k, t) \\\\\n",
    "&= \\max_{t \\in T_k} \\left( \\operatorname{Ent}(D) − \\frac{|D_{\\text{left}}^t|}{|D|} \\operatorname{Ent}(D_{\\text{left}}^t) − \\frac{|D_{\\text{right}}^t|}{|D|} \\operatorname{Ent}(D_{\\text{right}}^t) \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "其中，$T_k$ 表示平均值的集合，$D_{\\text{left/right}}^t$ 表示左右节点拥有的样本集（左侧为小于阈值，右侧大于阈值）。\n",
    "\n",
    "划分的目标是使得样本集按照某个特征的某个阈值划分之后的纯度提高，因此可找到最合适的划分特征及阈值，如下：\n",
    "\n",
    "$$k_{\\text{best}}, t_{\\text{best}} = \\underset{k, t}{\\operatorname{argmax}} \\operatorname{Gain}(D, k, t)$$\n",
    "\n",
    "##### 2. 基尼指数（Gini Index）\n",
    "\n",
    "基尼系数（Gini Coefficient）一般用于衡量数据的不纯度，基尼系数的值越小，数据的纯度越高，同质性越高。其定义式如下：\n",
    "\n",
    "$$\\operatorname{Gini}(D) = 1- \\sum_{k = 1}^{K} p_k^2$$\n",
    "\n",
    "其中 $D$ 表示样本集，$K$ 表示样本集分类数，$p_k$ 表示第 $k$ 类样本在样本集所占比例。\n",
    "\n",
    "与信息增益中一致，离散特征可以根据其取值将样本集划分为不同的子集。定义基尼指数（Gini Index）为子集基尼系数的加权平均，其一般用于衡量一个随机选中的样本被错误分类的概率。因此，基尼指数越大，划分效果越差。\n",
    "\n",
    "基尼系数的定义式如下：\n",
    "\n",
    "$$\\operatorname{Gini}(D, k)= \\sum_{j = 1}^{M} \\frac{\\left| D^{j} \\right|}{|D|} \\operatorname{Gini} \\left(D^{j} \\right)$$\n",
    "\n",
    "其中 $D$ 表示样本集，$k$ 表示离散属性，$M$ 表示离散属性 $k$ 所有可能取值的数量，$D^j$ 表示样本集中第 $j$ 种取值的子样本集。\n",
    "\n",
    "对于连续型属性，将样本集排序后俩俩取均值作为划分点，改写上式如下（只测试一个特征的一个阈值）：\n",
    "\n",
    "$$\\operatorname{Gini}(D, k)= \\max_{t \\in T_k} \\left( \\frac{|D_{\\text{left}}^t|}{|D|} \\operatorname{Gini}(D_{\\text{left}}^t) + \\frac{|D_{\\text{right}}^t|}{|D|} \\operatorname{Gini}(D_{\\text{right}}^t) \\right)$$\n",
    "\n",
    "其中 $T_k$ 表示平均值集合，$D_{\\text{left/right}}^t$ 表示左右子节点拥有的样本集（左侧小于阈值，右侧大于阈值）。\n",
    "\n",
    "划分的目标是使得样本集按照某个特征的某个阈值划分之后的纯度提高，因此可找到最合适的划分特征及阈值，如下：\n",
    "\n",
    "$$k_{\\text{best}}, t_{\\text{best}} = \\underset{k, t}{\\operatorname{argmin}} \\operatorname{Gini}(D, k, t)$$\n",
    "\n",
    "##### 3. 均方误差（MSE）\n",
    "\n",
    "使用前两种指标的决策树可以用来做分类问题，而想要让决策树可以用来做回归问题，就需要不同的指标来决定划分的特征。\n",
    "\n",
    "这个指标就是均方误差（MSE），其定义式如下：\n",
    "\n",
    "$$\\operatorname{MSE}(D) = \\frac{1}{N}\\sum_{i = 1}^{N}{\\left(y_{i} - \\hat{y}\\right)^{2}}$$\n",
    "\n",
    "其中 $D$ 为样本集；$i$ 表示样本集中的第 $i$ 个样本；$N$ 为样本个数；$y_i$ 表示第 $i$ 个样本的标签值；$\\hat{y}$ 表示样本标签的均值。\n",
    "\n",
    "$\\operatorname{MSE}(D)$ 的值越小，样本集的值越集中，将这些值视为一类也就更加合理。\n",
    "\n",
    "对于回归问题，样本的标签是一个连续的变量，因此很大程度上各个样本的标签都不相等，使用上面两种度量方式就可能导致计算出来的纯度指标并没有参考性（都差不多且纯度很低）.\n",
    "\n",
    "同样的，在中间节点我们可以根据变量的值将样本集划分为多个子集（一般为两个），以节点的 MSE 作为最优划分的度量标准。\n",
    "\n",
    "其定义如下：\n",
    "\n",
    "$$\\operatorname{MSE}(D, k)= \\sum_{j = 1}^{M} \\frac{\\left| D^{j} \\right|}{|D|} \\operatorname{MSE} \\left(D^{j} \\right)$$\n",
    "\n",
    "其中 $D$ 表示样本集，$k$ 表示离散属性，$M$ 表示离散属性 $k$ 所有可能取值的数量，$D^j$ 表示样本集中第 $j$ 种取值的子样本集。\n",
    "\n",
    "$operatorname{MSE}(D, k)$ 的值越小，决策树对样本集的拟合程度越高。因此，可找到最合适的划分特征及阈值如下：\n",
    "\n",
    "$$k_{\\text{best}}, t_{\\text{best}} = \\underset{k, t}{\\operatorname{argmin}} \\operatorname{MSE}(D, k, t)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
