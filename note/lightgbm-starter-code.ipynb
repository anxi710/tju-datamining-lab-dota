{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cb7ea607b69e92959e3a8c61ba4eba5ea76f9668"
   },
   "source": [
    "## In this kernel you will learn how to implement LightGBM + Kfold technique which results in higher score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "78d42119136692eeb3e1c4d1d8cf46281e5dd473"
   },
   "source": [
    "**LightGBM** is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n",
    "\n",
    "* Faster training speed and higher efficiency.\n",
    "* Lower memory usage.\n",
    "* Better accuracy.\n",
    "* Support of parallel and GPU learning.\n",
    "* Capable of handling large-scale data.\n",
    "\n",
    "You can read full documentation [here](https://lightgbm.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8f78dcc4f267c4d17def201a229453ec4945a26e"
   },
   "source": [
    "# here, imagine some cool picture about dota2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ff9e32f58f2700d539bad33f199a496a3c546a32"
   },
   "source": [
    "### Now, let's import all required packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os #to access files\n",
    "import pandas as pd #to work with dataframes\n",
    "import numpy as np #just a tradition\n",
    "from sklearn.model_selection import StratifiedKFold #for cross-validation\n",
    "from sklearn.metrics import roc_auc_score #this is we are trying to increase\n",
    "import matplotlib.pyplot as plt #we will plot something at the end)\n",
    "import seaborn as sns #same reason\n",
    "import lightgbm as lgb #the model we gonna use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "34b2c47bd550117380e4117ca51536d5f02716e4"
   },
   "source": [
    "## Let's read the data: train, target and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b7c2dbe68540821f1d00cd3c78d40f5345121a89"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "PATH_TO_DATA = '../data/'\n",
    "\n",
    "df_train_features = pd.read_csv(os.path.join(PATH_TO_DATA, \n",
    "                                             'train_features.csv'), \n",
    "                                    index_col='match_id_hash')\n",
    "df_train_targets = pd.read_csv(os.path.join(PATH_TO_DATA, \n",
    "                                            'train_targets.csv'), \n",
    "                                   index_col='match_id_hash')\n",
    "df_test_features = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_features.csv'), \n",
    "                                   index_col='match_id_hash')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "27477f5df270a6ebcd97d14185f453cfa9c66c7a"
   },
   "source": [
    "## Lets have a look what are these data look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c7ffd65c6ae9fa711dc479e3a5067b4994e8e193"
   },
   "outputs": [],
   "source": [
    "df_train_features.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1fb1cb6442b2efd1e4bdb25eebe6767c6a8b98f9"
   },
   "outputs": [],
   "source": [
    "df_train_targets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3a88575a73c0a701530a1b4088c16eb3ec63cce7"
   },
   "outputs": [],
   "source": [
    "df_test_features.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0713bf3a5ca9a0118a718d0a47ab9c6c1c059a57"
   },
   "source": [
    "I have no idea what these features mean...I prefer FIFA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8c2fdfb0a15ae061a9a7dbf5dd61d6bbbaed7645"
   },
   "outputs": [],
   "source": [
    "#turn to X and y notations for train data and target\n",
    "X = df_train_features.values\n",
    "y = df_train_targets['radiant_win'].values #extract the colomn we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "69c954edcefd01661c59fdfe4bf8e90906545173"
   },
   "outputs": [],
   "source": [
    "#this is to make sure we have \"ujson\" and \"tqdm\"\n",
    "#尝试导入上述两个库\n",
    "try:\n",
    "    import ujson as json\n",
    "except ModuleNotFoundError:\n",
    "    import json\n",
    "    print ('Please install ujson to read JSON oblects faster')\n",
    "    \n",
    "try:\n",
    "    from tqdm import tqdm_notebook\n",
    "except ModuleNotFoundError:\n",
    "    tqdm_notebook = lambda x: x\n",
    "    print ('Please install tqdm to track progress with Python loops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e33ab96c952dd661cc47a97cabd4b5e61f314335"
   },
   "outputs": [],
   "source": [
    "#a helper function, we will use it in next cell\n",
    "#一个函数，用于读取 matches_file（.jsonl 文件）并用进度条显示\n",
    "#可以分类讨论读取test_matches.jsonl和train_matches.jsonl两个文件\n",
    "\n",
    "def read_matches(matches_file):\n",
    "    \n",
    "    #一个字典，每种文件的条目数\n",
    "    MATCHES_COUNT = {\n",
    "        'test_matches.jsonl': 10000, \n",
    "        'train_matches.jsonl': 39675,\n",
    "    }\n",
    "    _, filename = os.path.split(matches_file)\n",
    "    total_matches = MATCHES_COUNT.get(filename)\n",
    "    \n",
    "    with open(matches_file) as fin:\n",
    "        for line in tqdm_notebook(fin, total=total_matches):\n",
    "            yield json.loads(line)\n",
    "            #逐行读取文件，并将每行的 JSON 数据解析为 Python 字典格式，通过 yield 返回每一条记录。\n",
    "            #yield 使得该函数成为生成器，按需返回数据，避免一次性读取大文件占用过多内存。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4bb72da4017bb4d62b69470bd09dfa838662220f"
   },
   "source": [
    "Now, we define a function which adds some new features:\n",
    "\n",
    "PS: all of these are from \"how to start\" kernel by @yorko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4ef48fcbe796559262ff6cdfb8161a657638b25c"
   },
   "outputs": [],
   "source": [
    "#函数，在读入的jsonl文件中，生成新的特征列\n",
    "\n",
    "def add_new_features(df_features, matches_file):\n",
    "    \n",
    "    # Process raw data and add new features\n",
    "    for match in read_matches(matches_file):\n",
    "        match_id_hash = match['match_id_hash']#对jsonl文件的每一行\n",
    "\n",
    "        # Counting ruined towers for both teams\n",
    "        #构建两个新的特征，构造他们的值\n",
    "        radiant_tower_kills = 0\n",
    "        dire_tower_kills = 0\n",
    "        for objective in match['objectives']:\n",
    "            if objective['type'] == 'CHAT_MESSAGE_TOWER_KILL':\n",
    "                if objective['team'] == 2:\n",
    "                    radiant_tower_kills += 1\n",
    "                if objective['team'] == 3:\n",
    "                    dire_tower_kills += 1\n",
    "\n",
    "        # Write new features\n",
    "        #将三个新的特征写入新的列\n",
    "        df_features.loc[match_id_hash, 'radiant_tower_kills'] = radiant_tower_kills\n",
    "        df_features.loc[match_id_hash, 'dire_tower_kills'] = dire_tower_kills\n",
    "        df_features.loc[match_id_hash, 'diff_tower_kills'] = radiant_tower_kills - dire_tower_kills\n",
    "        \n",
    "        #let's add one more\n",
    "        df_features.loc[match_id_hash, 'ratio_tower_kills'] = radiant_tower_kills / (0.01+dire_tower_kills)\n",
    "        # ... here you can add more features ...\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f3d2ac685d900d24201876aec37c8c7fde03400f"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#对训练集和测试集的特征文件，根据特征拓展函数，进行特征拓展操作\n",
    "\n",
    "# copy the dataframe with features\n",
    "df_train_features_extended = df_train_features.copy()\n",
    "df_test_features_extended = df_test_features.copy()\n",
    "\n",
    "# add new features\n",
    "add_new_features(df_train_features_extended, os.path.join(PATH_TO_DATA, 'train_matches.jsonl'))\n",
    "add_new_features(df_test_features_extended, os.path.join(PATH_TO_DATA, 'test_matches.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8678350cfa847dd5812802ea3fc52294d4036e13"
   },
   "outputs": [],
   "source": [
    "#Just a shorter names for data\n",
    "newtrain=df_train_features_extended #train集的X\n",
    "newtest=df_test_features_extended #test集的X\n",
    "target=pd.DataFrame(y) #用来监督的train集的标签，实际意义为胜率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2e3aa1b947cd07733abf9271eba904436be050f1"
   },
   "outputs": [],
   "source": [
    "#lastly, check the shapes, Andrew Ng approved)\n",
    "newtrain.shape,target.shape, newtest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dc9e54bfbe84bae0d828ce025fcb4440bf05e7af"
   },
   "source": [
    "After running the LightGBM model, we will visualize something called \"feature importance\", which  kind of shows which features and how much they affected the final result. For this reason we need to store feature names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "424e853ab70322ade3a028f6b026b73539dc4049"
   },
   "outputs": [],
   "source": [
    "#存储各个特征列名称，以便后续可视化展示个特征重要性\n",
    "features=newtrain.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "179b16f362643c804d21843c3ce9ad158674f833"
   },
   "source": [
    "## Noow, let's define LightGBM parameters. \n",
    "\n",
    "在前面完成了在csv文件中的数据读取操作；\n",
    "\n",
    "jsonl文件中的数据读取函数设计，新增特征函数设计\n",
    "\n",
    "并且对训练集和测试集的特征文件进行了特征拓展操作，作为新的训练集的X,y和测试集的X。。\n",
    "\n",
    "完成上述数据准备工作，现在可以进行模型的参数设计。\n",
    "\n",
    "Personally, I understand only some of these parameters. So, these are some random set up. Maybe it is better to look up the official documentation. Tuning these parameters definitely will increase your score.\n",
    "\n",
    "Investigation in process..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cf205faf4ea0bba22c37b0e00cdf75697d7ea102"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "param = {\n",
    "        'bagging_freq': 5,  #handling overfitting 每隔五轮从训练集中采样一次\n",
    "        'bagging_fraction': 0.5,  #handling overfitting - adding some noise 每次只采样50%来训练\n",
    "        'boost_from_average':'false', #如果设置为 true，则提升树会以目标值的平均值为基准。通常在分类问题中可能关闭此选项\n",
    "        'boost': 'gbdt', #梯度提升决策树算法\n",
    "        'feature_fraction': 0.05, #handling overfitting\n",
    "        'learning_rate': 0.01,  #the changes between one auc and a better one gets really small thus a small learning rate performs better\n",
    "        'max_depth': -1,  \n",
    "        'metric':'auc',\n",
    "        'min_data_in_leaf': 50,\n",
    "        'min_sum_hessian_in_leaf': 10.0,\n",
    "        'num_leaves': 10,\n",
    "        'num_threads': 5,\n",
    "        'tree_learner': 'serial', \n",
    "        'objective': 'binary', \n",
    "        'verbosity': 1\n",
    "    }\n",
    "'''\n",
    "\n",
    "\n",
    "param = {\n",
    "    'boosting_type' : 'gbdt', #梯度提升决策树，一种利用残差的多决策树集成学习\n",
    "    'objective' : 'binary', #二分类\n",
    "    'metric' : 'auc', #模型评估指标\n",
    "    #'num_iterations' : 100, #生成多少棵树，即追逐残差多少次\n",
    "    'learning_rate': 0.01,\n",
    "    #下面有控制决策树叶子结点最多最少，深度最多最少的参数，均采用default\n",
    "    'num_leaves' : 31, #最大叶子结点数\n",
    "    'max_depth' : -1, #不限制最大参数\n",
    "    'min_data_in_leaf' : 20, #一个叶子结点中最小的样本量，防止过度细的分类而过拟合\n",
    "    'feature_fraction' : 0.9, #每次构建树时用于选择的特征的比例，不选择1防止过拟合\n",
    "    'bagging_fraction' : 0.8, #每次迭代时用于训练的数据的比例，不选择1(全部样本)防止过拟合\n",
    "    'verbosity': 1 #控制训练过程中的输出级别，表示要输出信息\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "adaaf440f3ee9376b6d0738b409c05479133d145"
   },
   "source": [
    "# Finally, let's run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a715ea890bc3b4d64e1aa3f2ca807416ddd4f868"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#divide training data into train and validaton folds\n",
    "#folds = StratifiedKFold(n_splits=5, shuffle=False, random_state=17) #不打乱数据顺序 5折划分\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=False)\n",
    "\n",
    "#placeholder for out-of-fold, i.e. validation scores\n",
    "#初始化结果的占位符\n",
    "oof = np.zeros(len(newtrain))#39675 承接 在初始训练集上的预测的结果 的容器\n",
    "\n",
    "#for predictions\n",
    "predictions = np.zeros(len(newtest))#10000 承接在初始测试集上的预测结果的容器\n",
    "\n",
    "#and for feature importance\n",
    "feature_importance_df = pd.DataFrame() #每个特征的重要性\n",
    "\n",
    "#RUN THE LOOP OVER FOLDS\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(newtrain.values, target.values)):#划分训练集的X和y，选其中4折作为新训练集，1折作为新测试集\n",
    "    #trn_idx, val_idx ：划分为训练集和测试集的样本行数索引\n",
    "    \n",
    "    X_train, y_train = newtrain.iloc[trn_idx], target.iloc[trn_idx]\n",
    "    X_valid, y_valid = newtrain.iloc[val_idx], target.iloc[val_idx]\n",
    "    \n",
    "    print(\"Computing Fold {}\".format(fold_))\n",
    "    #把原先的训练集new_train进一步五折划分为训练集和测试集，以便交叉训练\n",
    "    trn_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "\n",
    "    \n",
    "    num_round = 5000 \n",
    "    verbose=1000 \n",
    "    stop=500 \n",
    "    \n",
    "    # 使用回调函数实现 early_stopping 和控制日志输出\n",
    "    callbacks = [\n",
    "        lgb.early_stopping(stopping_rounds=stop),  # 500轮内验证集性能指标无明显改善，则终止训练\n",
    "        lgb.log_evaluation(period=verbose)  # 每隔多少轮输出一下训练日志\n",
    "    ]\n",
    "\n",
    "    #TRAIN THE MODEL\n",
    "    clf = lgb.train(param, trn_data, \n",
    "                    num_round, #最大迭代轮次\n",
    "                    valid_sets = [trn_data, val_data], #训练集和验证集一起作为验证\n",
    "                    valid_names=['train', 'valid'],  # 添加验证集名称\n",
    "                    callbacks=callbacks  # 使用回调函数实现功能\n",
    "                    )\n",
    "    \n",
    "    #CALCULATE PREDICTION FOR VALIDATION SET\n",
    "    #在5折划分中，验证集的那一折上面，用训练好的模型预测\n",
    "    oof[val_idx] = clf.predict(newtrain.iloc[val_idx], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    #FEATURE IMPORTANCE\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()#根据特征被用于分割节点的次数计算各属性重要性\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1 #记录本次是第几轮得到的重要性\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0) #将本次得到的各属性重要性与之前的作一个表格拼接\n",
    "    #最终得到一个三列的表格：属性|重要性|第几折得到的这个重要性\n",
    "    \n",
    "    #CALCULATE PREDICTIONS FOR TEST DATA, using best_iteration on the fold\n",
    "    predictions += clf.predict(newtest, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "#print overall cross-validatino score\n",
    "print(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0860209942b4c946ca2563df688f7aef8362d64e"
   },
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "57be0c62e28a30693068b62757b2bd9930d4fe23"
   },
   "outputs": [],
   "source": [
    "# 计算每个特征在所有折中的平均重要性，并按重要性降序排列，选择前150个特征的索引\n",
    "\n",
    "cols = (feature_importance_df[[\"Feature\", \"importance\"]] #选取关心的两列\n",
    "        .groupby(\"Feature\") #一个groupby\n",
    "        .mean() #取平均\n",
    "        .sort_values(by=\"importance\", ascending=False)[:150].index)\n",
    "# 在所有特征中，筛选出属于前150个特征的行，存储为 best_features\n",
    "best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,28))\n",
    "sns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\n",
    "plt.title('Features importance (averaged/folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('FI.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "17f629257a6b2f82c745f17665198c879b4d1c99"
   },
   "source": [
    "From these feature importance chart, we can see that some features play significant role in making the prediction than others. Maybe dropping out less affecting features is a good idea. But still need more investigation of dota2 features..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "da024f725a21ffa48e15b057b0e9c3ed478315cd"
   },
   "source": [
    "## Prepare submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9714e8c941c7b2d9cc07a1765657fb75d8c8d287"
   },
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame({'radiant_win_prob': predictions}, \n",
    "                                 index=df_test_features.index)\n",
    "import datetime\n",
    "submission_filename = 'submission_{}.csv'.format(\n",
    "    datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "df_submission.to_csv(submission_filename)\n",
    "print('Submission saved to {}'.format(submission_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "942385e3bd4bc8d8a0fe0ff6d8060a850797733e"
   },
   "source": [
    "# What's next?\n",
    "\n",
    "* try to tune parameters, it will definitely improve your LB score\n",
    "* try to come up with good features\n",
    "* read other kernels\n",
    "* try other models as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "### Hopefully, this kernel was usefull. Feel free to fork, comment and upvote!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
