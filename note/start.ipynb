{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dota 2 Winner Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description\n",
    "\n",
    "We have the following files:\n",
    "\n",
    "- sample_submission.csv: example of a submission file\n",
    "\n",
    "- train_matches.jsonl, test_matches.jsonl: full \"raw\" training data\n",
    "\n",
    "- train_features.csv, test_features.csv: features created by organizers\n",
    "\n",
    "- train_targets.csv: results of training games (including the winner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features created by organizers\n",
    "\n",
    "These are basic features which include simple players' statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "PATH_TO_DATA = '../data/'\n",
    "\n",
    "df_train_features = pd.read_csv(os.path.join(PATH_TO_DATA,\n",
    "                                             'train_features.csv'),\n",
    "                                    index_col='match_id_hash')\n",
    "df_train_targets = pd.read_csv(os.path.join(PATH_TO_DATA,\n",
    "                                            'train_targets.csv'),\n",
    "                                   index_col='match_id_hash')\n",
    "\n",
    "df_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features[\"lobby_type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train_features.shape)\n",
    "print(df_train_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features.index.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have ~ 40k games, each described by match_id_hash (game id) and 245 features. Also game_time is given - time (in secs) when the snapshot was taken.\n",
    "\n",
    "duration_time - time (in secs) when the game was over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the radiant_win column in train_targets.csv. All these features are not known during the game (they come \"from future\" as compared to game_time), so we have these features only for training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以看到，获胜和失败的比例大约是1:1，所以这是一个平衡的二分类问题。\n",
    "df_train_targets['radiant_win'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features.isnull().sum().sum()  # no missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluating a model\n",
    "\n",
    "Let's construct a feature matrix X and a target vector y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train_features.values\n",
    "y = df_train_targets['radiant_win'].values\n",
    "\n",
    "\"\"\" Perform a train/test split (a simple validation scheme) \"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y,\n",
    "                                                      test_size=0.3,\n",
    "                                                      random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# magic command to measure the execution time of the cell\n",
    "# (works only in Jupyter Notebook or IPython)\n",
    "# must be the first line of the cell\n",
    "\n",
    "\"\"\" Train the Random Forest model \"\"\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100, n_jobs=4, random_state=17)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make predictions for the holdout set.\n",
    "We need to predict probabilities of class 1 - that Radiant wins,\n",
    "thus we need index 1 in the matrix returned by the predict_proba method.\n",
    "\"\"\"\n",
    "y_pred = model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's evaluate prediction quality with the holdout set\n",
    "We'll calculate ROC-AUC.\n",
    "\"\"\"\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "valid_score = roc_auc_score(y_valid, y_pred)\n",
    "print('Validation ROC-AUC score:', valid_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Out of curiosity, we can calculate accuracy of a classifier which predicts class 1\n",
    "if predicted probability is higher than 50%.\n",
    "\"\"\"\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "valid_accuracy = accuracy_score(y_valid, y_pred > 0.5)\n",
    "print('Validation accuracy of P>0.5 classifier:', valid_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing a submission\n",
    "\n",
    "Now the same for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_features = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_features.csv'),\n",
    "                                   index_col='match_id_hash')\n",
    "\n",
    "X_test = df_test_features.values\n",
    "y_test_pred = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "df_submission = pd.DataFrame({'radiant_win_prob': y_test_pred},\n",
    "                                 index=df_test_features.index)\n",
    "\n",
    "df_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save the submission file, it's handy to include current datetime in the filename.\n",
    "\"\"\"\n",
    "import datetime\n",
    "\n",
    "submission_filename = f\"submission_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.csv\"\n",
    "\n",
    "df_submission.to_csv(submission_filename)\n",
    "\n",
    "print(f\"Submission saved to {submission_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "As we already know, cross-validation is a more reliable validation technique than just one train/test split. Here we'll resort to ShuffleSplit to create 5 70%/30% splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit, KFold\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=17)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run cross-validation\n",
    "\n",
    "We'll train 2 versions of the RandomForestClassifier model - first with default capacity (trees are not limited in depth), second - with min_samples_leaf=3, i.e. each leave is obliged to have at least 3 instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_rf1 = RandomForestClassifier(n_estimators=100, n_jobs=4,\n",
    "                                   max_depth=None, random_state=17)\n",
    "\n",
    "# calcuate ROC-AUC for each split\n",
    "cv_scores_rf1 = cross_val_score(model_rf1, X, y, cv=cv, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_rf2 = RandomForestClassifier(n_estimators=100, n_jobs=4,\n",
    "                                   min_samples_leaf=3, random_state=17)\n",
    "\n",
    "cv_scores_rf2 = cross_val_score(model_rf2, X, y, cv=cv,\n",
    "                                scoring='roc_auc', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV results\n",
    "\n",
    "The result returned by cross_val_score is an array with metric values (ROC-AUC) for each split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_scores_rf1)\n",
    "\n",
    "print(cv_scores_rf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare average ROC-AUC among all splits for both models.\n",
    "\n",
    "print('Model 1 mean score:', cv_scores_rf1.mean())\n",
    "print('Model 2 mean score:', cv_scores_rf2.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second model is preferred. Look, there's a caveat here: the second model is actually better for 4 splits out of 5. So if we were to perform only one train/test split, there would've been a 20% probability to make a wrong conclusion that the first model is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores_rf2 > cv_scores_rf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with all available information on Dota games\n",
    "\n",
    "Raw data descriptions for all games are given in files train_matches.jsonl and test_matches.jsonl. Each file has one entry for each game in JSON format. You only need to know that it can be easily converted to Python objects via the json.loads method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore a single entry\n",
    "import json\n",
    "\n",
    "with open(os.path.join(PATH_TO_DATA, 'train_matches.jsonl')) as fin:\n",
    "    # read the 18-th line\n",
    "    for i in range(18):\n",
    "        line = fin.readline()\n",
    "\n",
    "    # read JSON into a Python object\n",
    "    match = json.loads(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `match` object is now a big Python dictionary. In `match['players']` we have a description of each player.\n",
    "\n",
    "You might think that this match object look ugly. You're right! That's actually the real data. And it's the ability to extract nice features from raw data that makes good Data Scientists stand out. You might even be unfamiliar with Dota (or any other application domain) but still be able to construct a good model via feature engineering. It's art and craftmanship at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Player description\n",
    "player = match['players'][2]\n",
    "\n",
    "# KDA: the number of kills, deaths, and assists to alleys.\n",
    "print(player['kills'], player['deaths'], player['assists'])\n",
    "\n",
    "# Some statistics on player abilities:\n",
    "print(player['ability_uses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: time series for each player's gold.\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "for player in match['players']:\n",
    "    plt.plot(player['times'], player['gold_t'])\n",
    "\n",
    "plt.title('Gold change for all players')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to read files with game descriptions\n",
    "\n",
    "The following function `read_matches(filename)`, can be used to read raw data on Dota 2 games.\n",
    "\n",
    "We recommend to install two Python packages: `ujson` and `tqdm`, it'll make the execution faster and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    import ujson as json\n",
    "except ModuleNotFoundError:\n",
    "    import json\n",
    "    print ('Please install ujson to read JSON oblects faster')\n",
    "\n",
    "try:\n",
    "    from tqdm import notebook\n",
    "except ModuleNotFoundError:\n",
    "    notebook.tqdm = lambda x: x\n",
    "    print ('Please install tqdm to track progress with Python loops')\n",
    "\n",
    "def read_matches(matches_file):\n",
    "\n",
    "    MATCHES_COUNT = {\n",
    "        'test_matches.jsonl': 10000,\n",
    "        'train_matches.jsonl': 39675,\n",
    "    }\n",
    "    _, filename = os.path.split(matches_file)\n",
    "    total_matches = MATCHES_COUNT.get(filename)\n",
    "\n",
    "    with open(matches_file) as fin:\n",
    "        for line in notebook.tqdm(fin, total=total_matches):\n",
    "            yield json.loads(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data in a loop\n",
    "\n",
    "Reading data on all games might take some 2-3 minutes. Thus you'd better stick to the following approach:\n",
    "\n",
    "1. Read a small amount (10-100) of games\n",
    "\n",
    "2. Write code to extract features from these JSON objects\n",
    "\n",
    "3. Make sure the code works fine\n",
    "\n",
    "4. Run the code with all available data\n",
    "\n",
    "5. Save results to a pickle file so that you don't need to run all computations from scratch next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "game_mode = []\n",
    "for match in read_matches(os.path.join(PATH_TO_DATA, 'train_matches.jsonl')):\n",
    "    # match_id_hash = match['match_id_hash']\n",
    "    # game_time = match['game_time']\n",
    "\n",
    "    # # processing each game\n",
    "\n",
    "    # for player in match['players']:\n",
    "    #     pass  # processing each player\n",
    "\n",
    "    game_mode.append(match['game_mode'])\n",
    "\n",
    "print(np.unique(game_mode, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "game_mode = []\n",
    "for match in read_matches(os.path.join(PATH_TO_DATA, 'train_matches.jsonl')):\n",
    "    game_mode.append(match['game_mode'])\n",
    "\n",
    "print(\"train data: \", np.unique(game_mode, return_counts=True))\n",
    "\n",
    "game_mode = []\n",
    "for match in read_matches(os.path.join(PATH_TO_DATA, 'test_matches.jsonl')):\n",
    "    game_mode.append(match['game_mode'])\n",
    "\n",
    "print(\"test data: \", np.unique(game_mode, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_features(df_features, matches_file):\n",
    "\n",
    "    # Process raw data and add new features\n",
    "    for match in read_matches(matches_file):\n",
    "        match_id_hash = match['match_id_hash']\n",
    "\n",
    "        # Counting ruined towers for both teams\n",
    "        radiant_tower_kills = 0\n",
    "        dire_tower_kills = 0\n",
    "        for objective in match['objectives']:\n",
    "            if objective['type'] == 'CHAT_MESSAGE_TOWER_KILL':\n",
    "                if objective['team'] == 2:\n",
    "                    radiant_tower_kills += 1\n",
    "                if objective['team'] == 3:\n",
    "                    dire_tower_kills += 1\n",
    "\n",
    "        # Write new features\n",
    "        df_features.loc[match_id_hash, 'radiant_tower_kills'] = radiant_tower_kills\n",
    "        df_features.loc[match_id_hash, 'dire_tower_kills'] = dire_tower_kills\n",
    "        df_features.loc[match_id_hash, 'diff_tower_kills'] = radiant_tower_kills - dire_tower_kills\n",
    "\n",
    "        # ... here you can add more features ...\n",
    "\n",
    "# copy the dataframe with features\n",
    "df_train_features_extended = df_train_features.copy()\n",
    "\n",
    "# add new features\n",
    "add_new_features(df_train_features_extended,\n",
    "                 os.path.join(PATH_TO_DATA,\n",
    "                              'train_matches.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see new features added to the right.\n",
    "df_train_features_extended.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating new features\n",
    "\n",
    "Let's run cross-validation with a fixed model but with two different datasets:\n",
    "\n",
    "1. with features built by organizers (base)\n",
    "\n",
    "2. with new features that we've added (extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, n_jobs=4, random_state=17)\n",
    "\n",
    "cv_scores_base = cross_val_score(model, X, y, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "cv_scores_extended = cross_val_score(model, df_train_features_extended.values, y,\n",
    "                                     cv=cv, scoring='roc_auc', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Base features: mean={} scores={}'.format(cv_scores_base.mean(),\n",
    "                                                cv_scores_base))\n",
    "print('Extended features: mean={} scores={}'.format(cv_scores_extended.mean(),\n",
    "                                                    cv_scores_extended))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores_extended > cv_scores_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, RandomForestClassifier shows better cross-validation results in case of the extended dataset. Looks reasonable, that's what we build features for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"New submission\"\"\"\n",
    "\n",
    "# Build the same features for the test set\n",
    "df_test_features_extended = df_test_features.copy()\n",
    "add_new_features(df_test_features_extended,\n",
    "                 os.path.join(PATH_TO_DATA, 'test_matches.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, n_jobs=4, random_state=17)\n",
    "model.fit(X, y)\n",
    "df_submission_base = pd.DataFrame(\n",
    "    {'radiant_win_prob': model.predict_proba(df_test_features.values)[:, 1]},\n",
    "    index=df_test_features.index,\n",
    ")\n",
    "df_submission_base.to_csv('submission_base_rf.csv')\n",
    "\n",
    "model_extended = RandomForestClassifier(n_estimators=100, n_jobs=4, random_state=17)\n",
    "model_extended.fit(df_train_features_extended.values, y)\n",
    "df_submission_extended = pd.DataFrame(\n",
    "    {'radiant_win_prob': model_extended.predict_proba(df_test_features_extended.values)[:, 1]}, \n",
    "    index=df_test_features.index,\n",
    ")\n",
    "df_submission_extended.to_csv('submission_extended_rf.csv')\n",
    "\n",
    "# this one will be used as a final submission in this kernel\n",
    "!cp submission_extended_rf.csv submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to build initial features from scratch\n",
    "\n",
    "Now we diclose the code that we used to build initial features `train_features.csv` and `test_features.csv`. You can modify the following code to add more features.\n",
    "\n",
    "In a nutshell:\n",
    "\n",
    "1. the `extract_features_csv(match)` function extracts features from game descriptions and writes them into a dictionary\n",
    "\n",
    "2. the `extract_targets_csv(match, targets)` function extracts the target variable `radiant_win`\n",
    "\n",
    "3. iterating through the file with raw data, we collect all features\n",
    "\n",
    "4. with `pandas.DataFrame.from_records()` we create dataframes with new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "MATCH_FEATURES = [\n",
    "    ('game_time', lambda m: m['game_time']),\n",
    "    ('game_mode', lambda m: m['game_mode']),\n",
    "    ('lobby_type', lambda m: m['lobby_type']),\n",
    "    ('objectives_len', lambda m: len(m['objectives'])),\n",
    "    ('chat_len', lambda m: len(m['chat'])),\n",
    "]\n",
    "\n",
    "PLAYER_FIELDS = [\n",
    "    'hero_id',\n",
    "\n",
    "    'kills',\n",
    "    'deaths',\n",
    "    'assists',\n",
    "    'denies',\n",
    "\n",
    "    'gold',\n",
    "    'lh',\n",
    "    'xp',\n",
    "    'health',\n",
    "    'max_health',\n",
    "    'max_mana',\n",
    "    'level',\n",
    "\n",
    "    'x',\n",
    "    'y',\n",
    "\n",
    "    'stuns',\n",
    "    'creeps_stacked',\n",
    "    'camps_stacked',\n",
    "    'rune_pickups',\n",
    "    'firstblood_claimed',\n",
    "    'teamfight_participation',\n",
    "    'towers_killed',\n",
    "    'roshans_killed',\n",
    "    'obs_placed',\n",
    "    'sen_placed',\n",
    "]\n",
    "\n",
    "def extract_features_csv(match):\n",
    "    row = [\n",
    "        ('match_id_hash', match['match_id_hash']),\n",
    "    ]\n",
    "\n",
    "    for field, f in MATCH_FEATURES:\n",
    "        row.append((field, f(match)))\n",
    "\n",
    "    for slot, player in enumerate(match['players']):\n",
    "        if slot < 5:\n",
    "            player_name = 'r%d' % (slot + 1)\n",
    "        else:\n",
    "            player_name = 'd%d' % (slot - 4)\n",
    "\n",
    "        for field in PLAYER_FIELDS:\n",
    "            column_name = '%s_%s' % (player_name, field)\n",
    "            row.append((column_name, player[field]))\n",
    "\n",
    "    return collections.OrderedDict(row)\n",
    "\n",
    "def extract_targets_csv(match, targets):\n",
    "    return collections.OrderedDict([('match_id_hash', match['match_id_hash'])] + [\n",
    "        (field, targets[field])\n",
    "        for field in ['game_time', 'radiant_win', 'duration', 'time_remaining', 'next_roshan_team']\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_new_features = []\n",
    "df_new_targets = []\n",
    "\n",
    "for match in read_matches(os.path.join(PATH_TO_DATA, 'train_matches.jsonl')):\n",
    "    match_id_hash = match['match_id_hash']\n",
    "    features = extract_features_csv(match)\n",
    "    targets = extract_targets_csv(match, match['targets'])\n",
    "\n",
    "    df_new_features.append(features)\n",
    "    df_new_targets.append(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_features = pd.DataFrame.from_records(df_new_features).set_index('match_id_hash')\n",
    "df_new_targets = pd.DataFrame.from_records(df_new_targets).set_index('match_id_hash')\n",
    "\n",
    "df_new_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go on!\n",
    "\n",
    "- Discuss new ideas in Slack\n",
    "\n",
    "- Create new features\n",
    "\n",
    "- Try new models and ensembles\n",
    "\n",
    "- Submit predictions\n",
    "\n",
    "- Go and win!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
