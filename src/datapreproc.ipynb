{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入预处理过程需要的所有软件包\n",
    "import os                  # 一些操作系统提供的 API\n",
    "from tqdm import notebook  # 为循环或其他迭代操作添加进度条\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ujson as json       # 用于读入 .json 文件\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part0. 分析特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_matches(matches_file):\n",
    "    \"\"\" 生成器函数，用于读取比赛数据 \"\"\"\n",
    "    MATCHES_COUNT = {\n",
    "        'test_matches.jsonl': 10000,\n",
    "        'train_matches.jsonl': 39675\n",
    "    }\n",
    "    _, filename = os.path.split(matches_file)\n",
    "    total_matches = MATCHES_COUNT.get(filename)\n",
    "\n",
    "    with open(matches_file) as fin:\n",
    "        for line in notebook.tqdm(fin, total=total_matches):\n",
    "            yield json.loads(line)\n",
    "\n",
    "def extract_keys(data, parent_key=None, res=None):\n",
    "    \"\"\" 递归提取 JSON 对象中的键及其父子关系 \"\"\"\n",
    "    if res is None:\n",
    "        res = set()\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        for key, val in data.items():\n",
    "            current_key = f\"{parent_key}.{key}\" if parent_key else key\n",
    "            res.add((parent_key, key))\n",
    "            extract_keys(val, current_key, res)\n",
    "    elif isinstance(data, list):\n",
    "        for item in data:\n",
    "            extract_keys(item, parent_key, res)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_rel_set = set()\n",
    "\n",
    "for match in read_matches('../data/train_matches.jsonl'):\n",
    "    keys_rel_set.update(extract_keys(match))\n",
    "\n",
    "keys_rel_df = pd.DataFrame(keys_rel_set, columns=[\"parent\", \"child\"])\n",
    "\n",
    "keys_rel_df.fillna(\"NULL\", inplace=True)\n",
    "\n",
    "# 去除 child 为数字的行\n",
    "keys_rel_df = keys_rel_df[~keys_rel_df[\"child\"].str.isnumeric()]\n",
    "\n",
    "# 去除 parent 最后一个单元为数字的行\n",
    "keys_rel_df = keys_rel_df[~keys_rel_df[\"parent\"].str.split(\".\").apply(lambda x: x[-1]).str.isnumeric()]\n",
    "\n",
    "# 去除不重要的键\n",
    "keys_rel_df = keys_rel_df[keys_rel_df[\"parent\"].str.split(\".\").apply(lambda x: x[-1]) != \"ability_uses\"]\n",
    "keys_rel_df = keys_rel_df[keys_rel_df[\"parent\"].str.split(\".\").apply(lambda x: x[-1]) != \"item_uses\"]\n",
    "keys_rel_df = keys_rel_df[keys_rel_df[\"parent\"].str.split(\".\").apply(lambda x: x[-1]) != \"damage_inflictor\"]\n",
    "keys_rel_df = keys_rel_df[keys_rel_df[\"parent\"].str.split(\".\").apply(lambda x: x[-1]) != \"hero_hits\"]\n",
    "keys_rel_df = keys_rel_df[keys_rel_df[\"parent\"].str.split(\".\").apply(lambda x: x[-1]) != \"damage_inflictor_received\"]\n",
    "keys_rel_df = keys_rel_df[keys_rel_df[\"parent\"].str.split(\".\").apply(lambda x: x[-1]) != \"purchase\"]\n",
    "\n",
    "# 去除所有含有 \"npc_dota\" 的行\n",
    "keys_rel_df = keys_rel_df[~keys_rel_df[\"parent\"].str.contains(\"npc_dota\")]\n",
    "keys_rel_df = keys_rel_df[~keys_rel_df[\"child\"].str.contains(\"npc_dota\")]\n",
    "\n",
    "keys_rel = keys_rel_df.apply(lambda x: tuple(x), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(tuples):\n",
    "    \"\"\" 根据父子关系列表构建树结构 \"\"\"\n",
    "    tree = {}\n",
    "\n",
    "    for path, node in tuples:\n",
    "        if path == \"NULL\":\n",
    "            # 如果没有路径，直接在根级别创建该节点\n",
    "            if node not in tree:\n",
    "                tree[node] = {}  # 执行顺序不定！\n",
    "            continue\n",
    "\n",
    "        parts = path.split('.')  # 将路径按照 '.' 分割\n",
    "        current_level = tree     # 从树的根节点开始\n",
    "\n",
    "        # 遍历路径中的每一段，逐层创建节点\n",
    "        for part in parts:\n",
    "            if part not in current_level:\n",
    "                current_level[part] = {}  # 如果不存在该节点，创建一个字典表示该节点\n",
    "            current_level = current_level[part]  # 继续向下层递归\n",
    "\n",
    "        if node not in current_level:\n",
    "            current_level[node] = {}  # 将当前节点添加到树中\n",
    "\n",
    "    return tree\n",
    "\n",
    "keys_rel_tree = build_tree(keys_rel)\n",
    "\n",
    "# 将树结构转换为可读性强的 json 格式\n",
    "with open(\"./data/all_features_train.json\", \"w\") as fout:\n",
    "    json.dump(keys_rel_tree, fout, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(tree, depth=0, file=None):\n",
    "    \"\"\" dfs 遍历树并打印 \"\"\"\n",
    "    for key, val in tree.items():\n",
    "        print(f\"{'    ' * depth}{key}\", file=file)\n",
    "        print_tree(val, depth + 1, file)\n",
    "\n",
    "print_tree(keys_rel_tree, file=open(\"./data/features_tree_train.txt\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_json_with_descriptions(json_file, excel_file, output_file):\n",
    "    \"\"\" 将 .xlsx 文件中的特征描述添加到 JSON 文件对应的特征中 \"\"\"\n",
    "    # 读取 JSON 文件\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 读取 Excel 文件，第一列是 'feature'，第二列是 'description'，第三列是 'priority'\n",
    "    df = pd.read_excel(excel_file)\n",
    "\n",
    "    # priority: T1 > T2 > T3 > T4 > T5 > 0\n",
    "    df['priority'] = df['priority'].map({'T1': 5, 'T2': 4, 'T3': 3, 'T4': 2, 'T5': 1, 0: 0})\n",
    "\n",
    "    # 创建 feature 到 description 和 priority 的映射\n",
    "    # feature : { description : \"\", priority : \"\" }\n",
    "    feature_descriptions = {feature: {\"description\": description, \"priority\": priority} for feature, description, priority in df.values}\n",
    "\n",
    "    # 递归更新 JSON 中的特征描述\n",
    "    def update_feature_descriptions(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            for key, value in obj.items():\n",
    "                if key in feature_descriptions and value == {}:  # 如果该特征的值为空\n",
    "                    obj[key] = feature_descriptions[key]\n",
    "                else:\n",
    "                    update_feature_descriptions(value)           # 递归处理子对象\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                update_feature_descriptions(item)\n",
    "\n",
    "    # 更新 JSON 数据\n",
    "    update_feature_descriptions(data)\n",
    "\n",
    "    # 将更新后的 JSON 数据保存到新文件中\n",
    "    with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "        json.dump(data, f_out, indent=4, ensure_ascii=False)\n",
    "\n",
    "json_file_path = \"./data/all_features_test.json\"   # 原始 JSON 文件路径\n",
    "excel_file_path = \"./data/dota-feature.xlsx\"       # Excel 文件路径\n",
    "output_file_path = \"./data/updated_features.json\"  # 输出的 JSON 文件路径\n",
    "\n",
    "update_json_with_descriptions(json_file_path, excel_file_path, output_file_path)\n",
    "\n",
    "# 将 *.0 替换为 *\n",
    "!sed -i 's/\\([0-9]\\)\\.0/\\1/g' ./data/updated_features.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "手工标注 updated_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1. 特征选择\n",
    "\n",
    "理解特征含义后，选择需要的特征并从原始数据中提取所需数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取原始数据方式如下\n",
    "for match in read_matches('../data/train_matches.jsonl'):\n",
    "    match_id_hash = match['match_id_hash']\n",
    "    game_time     = match['game_time']\n",
    "    teamfights    = match['teamfights']\n",
    "    objectives    = match['objectives']\n",
    "    game_mode     = match['game_mode']\n",
    "    players       = match['players']\n",
    "    print(type(match))       # class 'dict'\n",
    "    print(type(players))     # class 'list'\n",
    "    print(type(players[0]))  # class 'dict'\n",
    "    # To be continued...\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for match in read_matches('../data/train_matches.jsonl'):\n",
    "    teamfights = match['teamfights']\n",
    "    for teamfight in teamfights:\n",
    "        players = teamfight['players']\n",
    "        for player in players:\n",
    "            deaths_pos = player['deaths_pos']\n",
    "            if len(deaths_pos) > 0:\n",
    "                print(deaths_pos, match['match_id_hash'])\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_features(df_features, matches_file):\n",
    "\n",
    "    # Process raw data and add new features\n",
    "    for match in read_matches(matches_file):\n",
    "        match_id_hash = match['match_id_hash'] # 对 jsonl 文件的每一行\n",
    "\n",
    "        # Counting ruined towers for both teams\n",
    "        #构建两个新的特征，构造他们的值\n",
    "        radiant_tower_kills = 0\n",
    "        dire_tower_kills = 0\n",
    "        for objective in match['objectives']:\n",
    "            if objective['type'] == 'CHAT_MESSAGE_TOWER_KILL':\n",
    "                if objective['team'] == 2:\n",
    "                    radiant_tower_kills += 1\n",
    "                if objective['team'] == 3:\n",
    "                    dire_tower_kills += 1\n",
    "\n",
    "        # Write new features\n",
    "        #将三个新的特征写入新的列\n",
    "        df_features.loc[match_id_hash, 'radiant_tower_kills'] = radiant_tower_kills\n",
    "        df_features.loc[match_id_hash, 'dire_tower_kills'] = dire_tower_kills\n",
    "        df_features.loc[match_id_hash, 'diff_tower_kills'] = radiant_tower_kills - dire_tower_kills\n",
    "\n",
    "        #let's add one more\n",
    "        df_features.loc[match_id_hash, 'ratio_tower_kills'] = radiant_tower_kills / (0.01+dire_tower_kills)\n",
    "        # ... here you can add more features ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "MATCH_FEATURES = [\n",
    "    ('game_time', lambda m: m['game_time']),\n",
    "    ('game_mode', lambda m: m['game_mode']),\n",
    "    ('lobby_type', lambda m: m['lobby_type']),\n",
    "    ('objectives_len', lambda m: len(m['objectives'])),\n",
    "    ('chat_len', lambda m: len(m['chat'])),\n",
    "]\n",
    "\n",
    "PLAYER_FIELDS = [\n",
    "    'hero_id',\n",
    "\n",
    "    'kills',\n",
    "    'deaths',\n",
    "    'assists',\n",
    "    'denies',\n",
    "\n",
    "    'gold',\n",
    "    'lh',\n",
    "    'xp',\n",
    "    'health',\n",
    "    'max_health',\n",
    "    'max_mana',\n",
    "    'level',\n",
    "\n",
    "    'x',\n",
    "    'y',\n",
    "\n",
    "    'stuns',\n",
    "    'creeps_stacked',\n",
    "    'camps_stacked',\n",
    "    'rune_pickups',\n",
    "    'firstblood_claimed',\n",
    "    'teamfight_participation',\n",
    "    'towers_killed',\n",
    "    'roshans_killed',\n",
    "    'obs_placed',\n",
    "    'sen_placed',\n",
    "]\n",
    "\n",
    "def extract_features_csv(match):\n",
    "    row = [\n",
    "        ('match_id_hash', match['match_id_hash']),\n",
    "    ]\n",
    "\n",
    "    for field, f in MATCH_FEATURES:\n",
    "        row.append((field, f(match)))\n",
    "\n",
    "    for slot, player in enumerate(match['players']):\n",
    "        if slot < 5:\n",
    "            player_name = 'r%d' % (slot + 1)\n",
    "        else:\n",
    "            player_name = 'd%d' % (slot - 4)\n",
    "\n",
    "        for field in PLAYER_FIELDS:\n",
    "            column_name = '%s_%s' % (player_name, field)\n",
    "            row.append((column_name, player[field]))\n",
    "\n",
    "    return collections.OrderedDict(row)\n",
    "\n",
    "def extract_targets_csv(match, targets):\n",
    "    return collections.OrderedDict([('match_id_hash', match['match_id_hash'])] + [\n",
    "        (field, targets[field])\n",
    "        for field in ['game_time', 'radiant_win', 'duration', 'time_remaining', 'next_roshan_team']\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_features = []\n",
    "df_new_targets = []\n",
    "\n",
    "for match in read_matches('../data/train_matches.jsonl'):\n",
    "    match_id_hash = match['match_id_hash']\n",
    "    features = extract_features_csv(match)\n",
    "    targets = extract_targets_csv(match, match['targets'])\n",
    "\n",
    "    df_new_features.append(features)\n",
    "    df_new_targets.append(targets)\n",
    "\n",
    "df_new_features = pd.DataFrame.from_records(df_new_features).set_index('match_id_hash')\n",
    "df_new_targets = pd.DataFrame.from_records(df_new_targets).set_index('match_id_hash')\n",
    "\n",
    "df_new_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2. 数据清洗与加工\n",
    "\n",
    "类别编码\n",
    "\n",
    "可能的数据标准化 / 归一化等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3. 特征工程\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析时序数据\n",
    "# 原始数据集中有部分以 _t 结尾的时序数据\n",
    "# 这些数据是在比赛的不同时间点记录的，我们可以使用这些数据来分析比赛的动态变化\n",
    "# 如：gold_t, lh_t, dn_t, xp_t 等\n",
    "# gold_t => 金币数，lh_t => 补刀数，dn_t => 反补数，xp_t => 经验值\n",
    "\n",
    "for match in read_matches('../data/train_matches.jsonl'):\n",
    "    for player in match['players']:\n",
    "        plt.plot(player['times'], player['gold_t'])\n",
    "    break\n",
    "\n",
    "plt.title('Gold change for all players')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 计算随机森林的特征重要性\n",
    "\n",
    "使用 `scikit-learn` 库的随机森林模型时，计算特征重要性非常简单。\n",
    "\n",
    "可以直接通过模型的 `feature_importances_` 属性来获取每个特征的重要性得分。\n",
    "\n",
    "这个属性会返回一个数组，数组中的每个元素对应特征的重要性分数，分数越高，表示该特征对模型越重要。\n",
    "\n",
    "#### 3.1.1 基于 Gini 不纯度或信息增益评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的库并加载数据\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 加载示例数据集（Iris数据集）\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 定义并训练随机森林模型\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 获取特征重要性\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "# 输出特征重要性\n",
    "for i, importance in enumerate(importances):\n",
    "    print(f\"Feature {i}: Importance {importance}\")\n",
    "\n",
    "# 使用 matplotlib 对特征重要性进行可视化，以帮助理解哪些特征对模型贡献最大\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# 特征名称\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# 对特征重要性进行排序\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# 可视化特征重要性\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), [feature_names[i] for i in indices], rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 基于 OOB 评估\n",
    "\n",
    "> OOB (Out of Bag)\n",
    "\n",
    "随机森林在训练过程中会为每棵树随机抽取部分样本（bootstrap采样），这意味着一部分样本不会被用于训练当前的树，这些样本称为 **OOB样本**。\n",
    "\n",
    "可以通过将某个特征在 OOB 样本中 **打乱**，然后观察模型的性能变化，来评估该特征对模型的影响。\n",
    "\n",
    "**实现步骤**：\n",
    "\n",
    "Step1. 随机森林模型的 OOB 设置\n",
    "\n",
    "在使用`scikit-learn`训练随机森林模型时，可以通过将`oob_score`参数设置为`True`来启用 OOB 误差计算。\n",
    "\n",
    "Step2. 特征打乱与性能对比\n",
    "\n",
    "对于每个特征，可以按照以下步骤来计算OOB误差变化：\n",
    "\n",
    "1. 基准 OOB 性能：首先在原始数据上计算 OOB 误差，作为基准。\n",
    "\n",
    "2. 特征打乱：将某个特征的值随机打乱，再计算新的 OOB 误差。\n",
    "\n",
    "3. 误差变化：比较打乱特征前后的 OOB 误差差值，误差增大越多，说明该特征对模型性能的贡献越大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 加载数据集\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 训练随机森林模型，启用 OOB 误差计算\n",
    "rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# 查看基于OOB的性能\n",
    "print(f\"OOB Score: {rf.oob_score_}\")  # rf.oob_score_ 表示模型在 OOB 样本上的基准性能。\n",
    "\n",
    "# 计算特征打乱后的 OOB 误差变化，观察模型性能的变化。\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def permutation_importance_oob(model, X_train, y_train):\n",
    "    \"\"\" 计算基于 OOB 误差的特征重要性。\n",
    "\n",
    "    parameter:\n",
    "        1. model   : 训练好的随机森林模型\n",
    "        2. X_train : 训练集特征\n",
    "        3. y_train : 训练集标签\n",
    "    \"\"\"\n",
    "    # 初始化变量\n",
    "    base_oob_score = model.oob_score_  # 基准OOB分数\n",
    "    feature_importances = np.zeros(X_train.shape[1])  # 保存特征重要性\n",
    "\n",
    "    # 遍历每个特征\n",
    "    for col in range(X_train.shape[1]):\n",
    "        X_train_permuted = X_train.copy()  # 创建训练集副本\n",
    "        np.random.shuffle(X_train_permuted[:, col])  # 随机打乱某个特征列\n",
    "\n",
    "        # 用打乱特征后的训练集重新计算OOB得分\n",
    "        model.fit(X_train_permuted, y_train)\n",
    "        oob_score_permuted = model.oob_score_\n",
    "\n",
    "        # 计算OOB误差的变化\n",
    "        feature_importances[col] = base_oob_score - oob_score_permuted  # 分数下降越多，特征越重要\n",
    "\n",
    "    return feature_importances\n",
    "\n",
    "# 调用函数计算特征重要性\n",
    "oob_importances = permutation_importance_oob(rf, X_train, y_train)\n",
    "\n",
    "# 输出结果\n",
    "for i, importance in enumerate(oob_importances):\n",
    "    print(f\"Feature {i} ({data.feature_names[i]}): OOB Importance {importance}\")\n",
    "\n",
    "# 可视化特征重要性\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"OOB Feature Importance\")\n",
    "plt.bar(range(X_train.shape[1]), oob_importances, align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), data.feature_names, rotation=45)\n",
    "plt.ylabel(\"OOB Score Decrease\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优点：\n",
    "\n",
    "- **模型无关**：虽然我们在这里使用的是随机森林模型，但这种OOB误差打乱方法可以应用于任何基于bootstrap的模型。\n",
    "\n",
    "- **直接评估**：相比于仅通过分裂节点的纯度变化来评估，基于OOB误差的方法能够直接反映特征对模型整体预测能力的影响。\n",
    "\n",
    "挑战：\n",
    "\n",
    "- **计算成本**：每次打乱特征后，都需要重新拟合模型并计算OOB分数。这对于大规模数据集或模型复杂度较高的情况，计算开销会较大。\n",
    "\n",
    "- **特征相关性问题**：如果两个特征高度相关，打乱其中一个特征后，另一个特征可能会“补偿”它的影响，这使得模型的性能下降幅度不明显，从而低估该特征的重要性。\n",
    "\n",
    "可以将OOB误差的重要性与**Gini不纯度减少**的特征重要性评分结合起来，综合评估模型中各特征的影响力。\n",
    "\n",
    "**基于 OOB 误差的特征重要性评估** 方法通过打乱某个特征并观察模型 OOB 得分的变化来衡量该特征的重要性。\n",
    "\n",
    "这种方法能直接反映特征对模型预测性能的影响，尤其适合随机森林模型。虽然计算复杂度较高，但它能提供更直观的特征评估方式，特别是在高维数据集中。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
