{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入预处理过程需要的所有软件包\n",
    "import os                  # 一些操作系统提供的 API\n",
    "import csv\n",
    "from tqdm import tqdm      # 为循环或其他迭代操作添加进度条\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ujson as json       # 用于读入 .json 文件\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 宏定义\n",
    "PATH_TO_RAW_DATA = \"../data\"\n",
    "PATH_TO_PROCESSED_DATA = \"./data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part0. 分析特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.readjsonl import read_matches\n",
    "from utils.getfeaturetree import get_keys_relation, build_tree, print_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取训练集中的所有键的关系\n",
    "key_rel_train = get_keys_relation(os.path.join(PATH_TO_RAW_DATA, \"train_matches.jsonl\"))\n",
    "tree = build_tree(key_rel_train)\n",
    "\n",
    "feature_tree_train_path = os.path.join(PATH_TO_PROCESSED_DATA, \"feature_tree_train.txt\")\n",
    "with open(feature_tree_train_path, \"w\") as f:\n",
    "    print_tree(tree, file=f)\n",
    "\n",
    "# 将树存储为 .json 文件\n",
    "feature_tree_train_json_path = os.path.join(PATH_TO_PROCESSED_DATA, \"feature_tree_train.json\")\n",
    "with open(feature_tree_train_json_path, \"w\") as f:\n",
    "    json.dump(tree, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取测试集中的所有键的关系\n",
    "key_rel_test = get_keys_relation(os.path.join(PATH_TO_RAW_DATA, \"test_matches.jsonl\"))\n",
    "tree = build_tree(key_rel_test)\n",
    "\n",
    "feature_tree_test_path = os.path.join(PATH_TO_PROCESSED_DATA, \"feature_tree_test.txt\")\n",
    "with open(feature_tree_test_path, \"w\") as f:\n",
    "    print_tree(tree, file=f)\n",
    "\n",
    "# 将树存储为 .json 文件\n",
    "feature_tree_test_json_path = os.path.join(PATH_TO_PROCESSED_DATA, \"feature_tree_test.json\")\n",
    "with open(feature_tree_test_json_path, \"w\") as f:\n",
    "    json.dump(tree, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_json_with_descriptions(json_file, excel_file, output_file):\n",
    "    \"\"\" 将 .xlsx 文件中的特征描述添加到 JSON 文件对应的特征中 \"\"\"\n",
    "    # 读取 JSON 文件\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 读取 Excel 文件，第一列是 'feature'，第二列是 'description'，第三列是 'priority'\n",
    "    df = pd.read_excel(excel_file)\n",
    "\n",
    "    # 创建 feature 到 description 的映射\n",
    "    # feature : { description : \"\" }\n",
    "    feature_descriptions = {feature: {\"description\": description} for feature, description, _ in df.values}\n",
    "\n",
    "    # 递归更新 JSON 中的特征描述\n",
    "    def update_feature_descriptions(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            for key, value in obj.items():\n",
    "                if key in feature_descriptions and value == {}:  # 如果该特征的值为空\n",
    "                    obj[key] = feature_descriptions[key]\n",
    "                else:\n",
    "                    update_feature_descriptions(value)           # 递归处理子对象\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                update_feature_descriptions(item)\n",
    "\n",
    "    # 更新 JSON 数据\n",
    "    update_feature_descriptions(data)\n",
    "\n",
    "    # 将更新后的 JSON 数据保存到新文件中\n",
    "    with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "        json.dump(data, f_out, indent=4, ensure_ascii=False)\n",
    "\n",
    "json_file_path = \"./data/feature_tree_test.json\"   # 原始 JSON 文件路径\n",
    "excel_file_path = \"./data/dota-feature.xlsx\"       # Excel 文件路径\n",
    "output_file_path = \"./data/updated_feature.json\"  # 输出的 JSON 文件路径\n",
    "\n",
    "update_json_with_descriptions(json_file_path, excel_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "手工标注 updated_features 得到 feature_description.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1. 特征选择\n",
    "\n",
    "理解特征含义后，选择需要的特征并从原始数据中提取所需数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确保输出路径存在\n",
    "extracted_data_path = os.path.join(PATH_TO_RAW_DATA, 'extracted_data')\n",
    "os.makedirs(extracted_data_path, exist_ok=True)  # 如果目录不存在，创建它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('1.一级标签提取 —— main table')\n",
    "\n",
    "def extract_non_nested_and_count_nested_keys(json_object):\n",
    "    \"\"\"\n",
    "    从JSON对象中提取所有不含嵌套属性的一级属性，\n",
    "    并对包含嵌套属性的一级属性统计其内部的数据数量。\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "\n",
    "    for key, value in json_object.items():\n",
    "        if not isinstance(value, (dict, list)):  # 如果属性不含嵌套（即不是dict或list），直接提取\n",
    "            data[key] = value\n",
    "        elif isinstance(value, list):  # 如果属性是列表，统计其长度并添加统计列\n",
    "            data[f\"{key}_number\"] = len(value)\n",
    "        elif isinstance(value, dict):  # 如果属性是字典类型，也记录其包含的键值数量\n",
    "            data[f\"{key}_number\"] = len(value)\n",
    "\n",
    "    return data\n",
    "\n",
    "def jsonl_to_csv(jsonl_file_path, csv_file_path):\n",
    "    \"\"\" 将 JSONL 文件转换为 CSV 文件，提取不含嵌套的一级属性，并统计嵌套属性的数量 \"\"\"\n",
    "\n",
    "    with open(jsonl_file_path, 'r') as jsonl_file, open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_writer = None\n",
    "        for line in jsonl_file:\n",
    "            json_object = json.loads(line.strip())\n",
    "            # 提取不含嵌套的一级属性和嵌套属性的数量\n",
    "            processed_data = extract_non_nested_and_count_nested_keys(json_object)\n",
    "\n",
    "            # 删除不需要的列\n",
    "            for col_to_remove in ['objectives_number', 'players_number', 'targets_number']:\n",
    "                processed_data.pop(col_to_remove, None)\n",
    "\n",
    "            # 初始化CSV写入器（仅在第一次循环时执行）\n",
    "            if csv_writer is None:\n",
    "                csv_writer = csv.DictWriter(csv_file, fieldnames=processed_data.keys())\n",
    "                csv_writer.writeheader()\n",
    "\n",
    "            # 写入CSV行\n",
    "            csv_writer.writerow(processed_data)\n",
    "\n",
    "jsonl_file_path = os.path.join(PATH_TO_RAW_DATA, \"train_matches.jsonl\")\n",
    "csv_file_path = os.path.join(PATH_TO_RAW_DATA, 'extracted_data/main_table_deleted.csv')\n",
    "jsonl_to_csv(jsonl_file_path, csv_file_path)\n",
    "\n",
    "print('1.一级标签提取——main table提取成功')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('2.提取objectives表格')\n",
    "\n",
    "def find_max_objectives(jsonl_file_path):\n",
    "    \"\"\"\n",
    "    遍历JSONL文件，找出所有行中objectives的最大数量。\n",
    "    \"\"\"\n",
    "    max_objectives = 0\n",
    "    with open(jsonl_file_path, 'r') as jsonl_file:\n",
    "        for line in jsonl_file:\n",
    "            json_object = json.loads(line.strip())\n",
    "            objectives_count = len(json_object.get(\"objectives\", []))\n",
    "            if objectives_count > max_objectives:\n",
    "                max_objectives = objectives_count\n",
    "    print(max_objectives)\n",
    "    return max_objectives\n",
    "\n",
    "def extract_objectives(json_object, max_objectives):\n",
    "    \"\"\"\n",
    "    从JSON对象中提取所有objectives数据，将每个objective展开成单独的列。\n",
    "    如果没有那么多objectives，则保留空值。\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    objectives = json_object.get(\"objectives\", [])\n",
    "    for i in range(1, max_objectives + 1):\n",
    "        if i <= len(objectives):\n",
    "            objective = objectives[i - 1]\n",
    "            for key in [\"time\", \"type\", \"player_slot\", \"team\", \"key\", \"slot\"]:\n",
    "                column_name = f\"objective-{i}-{key}\"\n",
    "                data[column_name] = objective.get(key, \"\")\n",
    "        else:\n",
    "            for key in [\"time\", \"type\", \"player_slot\", \"team\", \"key\", \"slot\"]:\n",
    "                column_name = f\"objective-{i}-{key}\"\n",
    "                data[column_name] = \"\"\n",
    "    return data\n",
    "\n",
    "def jsonl_to_csv(jsonl_file_path, csv_file_path):\n",
    "    \"\"\"\n",
    "    将JSONL文件转换为CSV文件，提取并展开每一行中的所有objective数据。\n",
    "    \"\"\"\n",
    "    # 第一次遍历：找到最大的objectives数量\n",
    "    max_objectives = find_max_objectives(jsonl_file_path)\n",
    "\n",
    "    with open(jsonl_file_path, 'r') as jsonl_file, open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_writer = None\n",
    "        for line in jsonl_file:\n",
    "            json_object = json.loads(line.strip())\n",
    "            # 提取并展开objectives数据\n",
    "            processed_data = extract_objectives(json_object, max_objectives=max_objectives)\n",
    "            # 初始化CSV写入器（仅在第一次循环时执行）\n",
    "            if csv_writer is None:\n",
    "                csv_writer = csv.DictWriter(csv_file, fieldnames=processed_data.keys())\n",
    "                csv_writer.writeheader()\n",
    "            # 写入CSV行\n",
    "            csv_writer.writerow(processed_data)\n",
    "\n",
    "jsonl_file_path = os.path.join(PATH_TO_RAW_DATA, \"train_matches.jsonl\")\n",
    "csv_file_path = os.path.join(PATH_TO_RAW_DATA, 'extracted_data/objective_table_deleted.csv')\n",
    "jsonl_to_csv(jsonl_file_path, csv_file_path)\n",
    "\n",
    "print('2.提取objectives表格提取成功')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('3.提取targets表格')\n",
    "\n",
    "def extract_radiant_win(json_object):\n",
    "    \"\"\" 从 JSON 对象中提取 radiant_win 属性 \"\"\"\n",
    "    data = {}\n",
    "    targets = json_object.get(\"targets\", {})\n",
    "    # 仅提取radiant_win字段\n",
    "    data[\"radiant_win\"] = targets.get(\"radiant_win\", \"\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def jsonl_to_csv(jsonl_file_path, csv_file_path):\n",
    "    \"\"\"\n",
    "    将JSONL文件转换为CSV文件，仅提取radiant_win字段。\n",
    "    \"\"\"\n",
    "    with open(jsonl_file_path, 'r') as jsonl_file, open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_writer = None\n",
    "        for line in jsonl_file:\n",
    "            json_object = json.loads(line.strip())\n",
    "            # 提取radiant_win字段\n",
    "            processed_data = extract_radiant_win(json_object)\n",
    "            # 初始化CSV写入器（仅在第一次循环时执行）\n",
    "            if csv_writer is None:\n",
    "                csv_writer = csv.DictWriter(csv_file, fieldnames=processed_data.keys())\n",
    "                csv_writer.writeheader()\n",
    "            # 写入CSV行\n",
    "            csv_writer.writerow(processed_data)\n",
    "\n",
    "jsonl_file_path = os.path.join(PATH_TO_RAW_DATA, \"train_matches.jsonl\")\n",
    "csv_file_path = os.path.join(PATH_TO_RAW_DATA, 'extracted_data/target_table_radiantwin.csv')\n",
    "jsonl_to_csv(jsonl_file_path, csv_file_path)\n",
    "print('3.targets表格提取成功')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('4.提取teamfights表格')\n",
    "\n",
    "def extract_teamfights(json_object):\n",
    "    \"\"\"\n",
    "    从JSON对象中提取teamfights数据，将每个teamfight和其下的player属性展开成单独的列。\n",
    "    删除players中的ability_uses, deaths, deaths_pos, item_uses, 和killed字段。\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    teamfights = json_object.get(\"teamfights\", [])\n",
    "\n",
    "    for i, teamfight in enumerate(teamfights, start=1):\n",
    "        # 提取teamfight的顶层字段\n",
    "        data[f\"teamfights-{i}-end\"] = teamfight.get(\"end\", \"\")\n",
    "        data[f\"teamfights-{i}-start\"] = teamfight.get(\"start\", \"\")\n",
    "        data[f\"teamfights-{i}-deaths\"] = teamfight.get(\"deaths\", \"\")\n",
    "        data[f\"teamfights-{i}-last_death\"] = teamfight.get(\"last_death\", \"\")\n",
    "\n",
    "        # 提取每个player的数据\n",
    "        players = teamfight.get(\"players\", [])\n",
    "        for j, player in enumerate(players, start=1):\n",
    "            # 删除不需要的字段，只保留以下字段\n",
    "            data[f\"teamfights-{i}-player-{j}-xp_delta\"] = player.get(\"xp_delta\", \"\")\n",
    "            data[f\"teamfights-{i}-player-{j}-damage\"] = player.get(\"damage\", \"\")\n",
    "            data[f\"teamfights-{i}-player-{j}-gold_delta\"] = player.get(\"gold_delta\", \"\")\n",
    "            data[f\"teamfights-{i}-player-{j}-healing\"] = player.get(\"healing\", \"\")\n",
    "            data[f\"teamfights-{i}-player-{j}-buybacks\"] = player.get(\"buybacks\", \"\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def jsonl_to_csv(jsonl_file_path, csv_file_path):\n",
    "    \"\"\"\n",
    "    将JSONL文件转换为CSV文件，提取并展开每一行中的所有teamfights数据。\n",
    "    \"\"\"\n",
    "    all_fieldnames = set()\n",
    "    max_players = 0\n",
    "\n",
    "    # 第一次遍历：确定最大玩家数量和完整字段名\n",
    "    with open(jsonl_file_path, 'r') as jsonl_file:\n",
    "        for line in jsonl_file:\n",
    "            json_object = json.loads(line.strip())\n",
    "            teamfights = json_object.get(\"teamfights\", [])\n",
    "            processed_data = extract_teamfights(json_object)\n",
    "            all_fieldnames.update(processed_data.keys())\n",
    "\n",
    "    # 排序字段名，确保一致性\n",
    "    all_fieldnames = sorted(all_fieldnames)\n",
    "\n",
    "    # 第二次遍历：将数据写入CSV文件\n",
    "    with open(jsonl_file_path, 'r') as jsonl_file, open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_writer = csv.DictWriter(csv_file, fieldnames=all_fieldnames)\n",
    "        csv_writer.writeheader()\n",
    "        for line in jsonl_file:\n",
    "            json_object = json.loads(line.strip())\n",
    "            processed_data = extract_teamfights(json_object, max_players)\n",
    "            csv_writer.writerow(processed_data)\n",
    "\n",
    "jsonl_file_path = os.path.join(PATH_TO_RAW_DATA, \"train_matches.jsonl\")\n",
    "csv_file_path = os.path.join(PATH_TO_RAW_DATA, 'extracted_data/teamfights_table_deleted.csv')\n",
    "jsonl_to_csv(jsonl_file_path, csv_file_path)\n",
    "\n",
    "print('4.teamfights表格提取成功')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('5.提取players表格')\n",
    "\n",
    "def flatten_dict(data, prefix=\"\"):\n",
    "    \"\"\"\n",
    "    仅递归展开嵌套字典中需要展开的字段，避免过度拉平，保留指定字段为普通字段。\n",
    "    如果字段是字典，则展开为`prefix-key`的形式，如果是其他类型，则直接保存为值。\n",
    "    \"\"\"\n",
    "    flat_data = {}\n",
    "    for key, value in data.items():\n",
    "        new_key = f\"{prefix}-{key}\" if prefix else key\n",
    "        if isinstance(value, dict):\n",
    "            for sub_key, sub_value in value.items():\n",
    "                flat_data[f\"{new_key}-{sub_key}\"] = sub_value\n",
    "        else:\n",
    "            flat_data[new_key] = value\n",
    "    return flat_data\n",
    "\n",
    "def extract_players(json_object):\n",
    "    \"\"\"\n",
    "    从JSON对象中提取players数据，将每个player和其下的多级属性展开成单独的列。\n",
    "    删除指定字段，保留需要展开的字段，并处理多级嵌套。\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    players = json_object.get(\"players\", [])\n",
    "\n",
    "    for i, player in enumerate(players, start=1):\n",
    "        player_data = {}\n",
    "\n",
    "        # 仅保留需要的字段\n",
    "        required_fields = [\n",
    "            \"sen_placed\", \"sen_left_log\", \"kills\", \"obs_left_log\",\n",
    "            \"max_hero_hit\", \"obs_log\", \"max_mana\", \"creeps_stacked\",\n",
    "            \"xp_reasons\", \"randomed\", \"towers_killed\", \"health\",\n",
    "            \"rune_pickups\", \"level\", \"stuns\", \"deaths\", \"gold\",\n",
    "            \"nearby_creep_death_count\", \"denies\", \"observers_placed\",\n",
    "            \"sen_log\", \"hero_id\", \"max_health\"\n",
    "        ]\n",
    "\n",
    "        for field in required_fields:\n",
    "            if field in player:\n",
    "                # 对 max_hero_hit 特殊处理，只保留 value 字段\n",
    "                if field == \"max_hero_hit\" and isinstance(player[field], dict):\n",
    "                    player_data[field] = player[field].get(\"value\", \"\")\n",
    "                else:\n",
    "                    player_data[field] = player[field]\n",
    "\n",
    "        # 展开需要的嵌套字典\n",
    "        flat_player_data = flatten_dict(player_data, prefix=f\"players-{i}\")\n",
    "        data.update(flat_player_data)\n",
    "\n",
    "    return data\n",
    "\n",
    "def jsonl_to_csv(jsonl_file_path, csv_file_path):\n",
    "    \"\"\"\n",
    "    将JSONL文件转换为CSV文件，提取并展开每一行中的所有players数据。\n",
    "    \"\"\"\n",
    "    all_fieldnames = set()\n",
    "\n",
    "    # 第一次遍历：确定完整的字段名\n",
    "    with open(jsonl_file_path, 'r') as jsonl_file:\n",
    "        for line in jsonl_file:\n",
    "            json_object = json.loads(line.strip())\n",
    "            processed_data = extract_players(json_object)\n",
    "            all_fieldnames.update(processed_data.keys())\n",
    "\n",
    "    # 排序字段名，确保一致性\n",
    "    all_fieldnames = sorted(all_fieldnames)\n",
    "\n",
    "    # 第二次遍历：将数据写入CSV文件\n",
    "    with open(jsonl_file_path, 'r') as jsonl_file, open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_writer = csv.DictWriter(csv_file, fieldnames=all_fieldnames)\n",
    "        csv_writer.writeheader()\n",
    "        for line in jsonl_file:\n",
    "            json_object = json.loads(line.strip())\n",
    "            processed_data = extract_players(json_object)\n",
    "            csv_writer.writerow(processed_data)\n",
    "\n",
    "jsonl_file_path = os.path.join(PATH_TO_RAW_DATA, \"train_matches.jsonl\")\n",
    "csv_file_path = os.path.join(PATH_TO_RAW_DATA, 'extracted_data/players_table_deleted.csv')\n",
    "jsonl_to_csv(jsonl_file_path, csv_file_path)\n",
    "\n",
    "print('5.players表格提取成功')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "To be continued...\n",
    "\n",
    "添加新的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_features(df_features, matches_file):\n",
    "\n",
    "    # Process raw data and add new features\n",
    "    for match in read_matches(matches_file):\n",
    "        match_id_hash = match['match_id_hash'] # 对 jsonl 文件的每一行\n",
    "\n",
    "        # Counting ruined towers for both teams\n",
    "        #构建两个新的特征，构造他们的值\n",
    "        radiant_tower_kills = 0\n",
    "        dire_tower_kills = 0\n",
    "        for objective in match['objectives']:\n",
    "            if objective['type'] == 'CHAT_MESSAGE_TOWER_KILL':\n",
    "                if objective['team'] == 2:\n",
    "                    radiant_tower_kills += 1\n",
    "                if objective['team'] == 3:\n",
    "                    dire_tower_kills += 1\n",
    "\n",
    "        # Write new features\n",
    "        #将三个新的特征写入新的列\n",
    "        df_features.loc[match_id_hash, 'radiant_tower_kills'] = radiant_tower_kills\n",
    "        df_features.loc[match_id_hash, 'dire_tower_kills'] = dire_tower_kills\n",
    "        df_features.loc[match_id_hash, 'diff_tower_kills'] = radiant_tower_kills - dire_tower_kills\n",
    "\n",
    "        #let's add one more\n",
    "        df_features.loc[match_id_hash, 'ratio_tower_kills'] = radiant_tower_kills / (0.01+dire_tower_kills)\n",
    "        # ... here you can add more features ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "MATCH_FEATURES = [\n",
    "    ('game_time', lambda m: m['game_time']),\n",
    "    ('game_mode', lambda m: m['game_mode']),\n",
    "    ('lobby_type', lambda m: m['lobby_type']),\n",
    "    ('objectives_len', lambda m: len(m['objectives'])),\n",
    "    ('chat_len', lambda m: len(m['chat'])),\n",
    "]\n",
    "\n",
    "PLAYER_FIELDS = [\n",
    "    'hero_id',\n",
    "\n",
    "    'kills',\n",
    "    'deaths',\n",
    "    'assists',\n",
    "    'denies',\n",
    "\n",
    "    'gold',\n",
    "    'lh',\n",
    "    'xp',\n",
    "    'health',\n",
    "    'max_health',\n",
    "    'max_mana',\n",
    "    'level',\n",
    "\n",
    "    'x',\n",
    "    'y',\n",
    "\n",
    "    'stuns',\n",
    "    'creeps_stacked',\n",
    "    'camps_stacked',\n",
    "    'rune_pickups',\n",
    "    'firstblood_claimed',\n",
    "    'teamfight_participation',\n",
    "    'towers_killed',\n",
    "    'roshans_killed',\n",
    "    'obs_placed',\n",
    "    'sen_placed',\n",
    "]\n",
    "\n",
    "def extract_features_csv(match):\n",
    "    row = [\n",
    "        ('match_id_hash', match['match_id_hash']),\n",
    "    ]\n",
    "\n",
    "    for field, f in MATCH_FEATURES:\n",
    "        row.append((field, f(match)))\n",
    "\n",
    "    for slot, player in enumerate(match['players']):\n",
    "        if slot < 5:\n",
    "            player_name = 'r%d' % (slot + 1)\n",
    "        else:\n",
    "            player_name = 'd%d' % (slot - 4)\n",
    "\n",
    "        for field in PLAYER_FIELDS:\n",
    "            column_name = '%s_%s' % (player_name, field)\n",
    "            row.append((column_name, player[field]))\n",
    "\n",
    "    return collections.OrderedDict(row)\n",
    "\n",
    "def extract_targets_csv(match, targets):\n",
    "    return collections.OrderedDict([('match_id_hash', match['match_id_hash'])] + [\n",
    "        (field, targets[field])\n",
    "        for field in ['game_time', 'radiant_win', 'duration', 'time_remaining', 'next_roshan_team']\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_features = []\n",
    "df_new_targets = []\n",
    "\n",
    "for match in read_matches('../data/train_matches.jsonl'):\n",
    "    match_id_hash = match['match_id_hash']\n",
    "    features = extract_features_csv(match)\n",
    "    targets = extract_targets_csv(match, match['targets'])\n",
    "\n",
    "    df_new_features.append(features)\n",
    "    df_new_targets.append(targets)\n",
    "\n",
    "df_new_features = pd.DataFrame.from_records(df_new_features).set_index('match_id_hash')\n",
    "df_new_targets = pd.DataFrame.from_records(df_new_targets).set_index('match_id_hash')\n",
    "\n",
    "df_new_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2. 数据清洗与加工\n",
    "\n",
    "类别编码\n",
    "\n",
    "可能的数据标准化 / 归一化等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "file_path_player_table = 'player_table_cleaning.csv'\n",
    "player_table_data = pd.read_csv(file_path_player_table)\n",
    "\n",
    "# 填充空值为 0\n",
    "player_table_data.fillna(0, inplace=True)\n",
    "\n",
    "# 替换布尔值\n",
    "player_table_data.replace({False: 0, True: 1}, inplace=True)\n",
    "\n",
    "\n",
    "# 遍历每个玩家的 xp_reasons 列进行处理\n",
    "for player_num in range(1, 11):\n",
    "    # 构造当前玩家的 xp_reasons 列名\n",
    "    reason_cols = [f'players-{player_num}-xp_reasons-{i}' for i in range(4)]\n",
    "    new_col_name = f'players-{player_num}-xp'  # 新列名\n",
    "\n",
    "    # 检查是否所有列存在\n",
    "    if all(col in player_table_data.columns for col in reason_cols):\n",
    "        # 计算总和\n",
    "        player_table_data[new_col_name] = player_table_data[reason_cols].sum(axis=1)\n",
    "\n",
    "        # 找到原始位置索引\n",
    "        first_col_index = player_table_data.columns.get_loc(reason_cols[0])\n",
    "\n",
    "        # 删除原始 xp_reasons 列\n",
    "        player_table_data.drop(columns=reason_cols, inplace=True)\n",
    "\n",
    "        # 调整列顺序，将新列插入到原始位置\n",
    "        cols = player_table_data.columns.tolist()\n",
    "        cols.insert(first_col_index, cols.pop(cols.index(new_col_name)))\n",
    "        player_table_data = player_table_data[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3. 特征工程\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析时序数据\n",
    "# 原始数据集中有部分以 _t 结尾的时序数据\n",
    "# 这些数据是在比赛的不同时间点记录的，我们可以使用这些数据来分析比赛的动态变化\n",
    "# 如：gold_t, lh_t, dn_t, xp_t 等\n",
    "# gold_t => 金币数，lh_t => 补刀数，dn_t => 反补数，xp_t => 经验值\n",
    "\n",
    "for match in read_matches('../data/train_matches.jsonl'):\n",
    "    for player in match['players']:\n",
    "        plt.plot(player['times'], player['gold_t'])\n",
    "    break\n",
    "\n",
    "plt.title('Gold change for all players')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 计算随机森林的特征重要性\n",
    "\n",
    "使用 `scikit-learn` 库的随机森林模型时，计算特征重要性非常简单。\n",
    "\n",
    "可以直接通过模型的 `feature_importances_` 属性来获取每个特征的重要性得分。\n",
    "\n",
    "这个属性会返回一个数组，数组中的每个元素对应特征的重要性分数，分数越高，表示该特征对模型越重要。\n",
    "\n",
    "#### 3.1.1 基于 Gini 不纯度或信息增益评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的库并加载数据\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 加载示例数据集（Iris数据集）\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 定义并训练随机森林模型\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 获取特征重要性\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "# 输出特征重要性\n",
    "for i, importance in enumerate(importances):\n",
    "    print(f\"Feature {i}: Importance {importance}\")\n",
    "\n",
    "# 使用 matplotlib 对特征重要性进行可视化，以帮助理解哪些特征对模型贡献最大\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# 特征名称\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# 对特征重要性进行排序\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# 可视化特征重要性\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), [feature_names[i] for i in indices], rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 基于 OOB 评估\n",
    "\n",
    "> OOB (Out of Bag)\n",
    "\n",
    "随机森林在训练过程中会为每棵树随机抽取部分样本（bootstrap采样），这意味着一部分样本不会被用于训练当前的树，这些样本称为 **OOB样本**。\n",
    "\n",
    "可以通过将某个特征在 OOB 样本中 **打乱**，然后观察模型的性能变化，来评估该特征对模型的影响。\n",
    "\n",
    "**实现步骤**：\n",
    "\n",
    "Step1. 随机森林模型的 OOB 设置\n",
    "\n",
    "在使用`scikit-learn`训练随机森林模型时，可以通过将`oob_score`参数设置为`True`来启用 OOB 误差计算。\n",
    "\n",
    "Step2. 特征打乱与性能对比\n",
    "\n",
    "对于每个特征，可以按照以下步骤来计算OOB误差变化：\n",
    "\n",
    "1. 基准 OOB 性能：首先在原始数据上计算 OOB 误差，作为基准。\n",
    "\n",
    "2. 特征打乱：将某个特征的值随机打乱，再计算新的 OOB 误差。\n",
    "\n",
    "3. 误差变化：比较打乱特征前后的 OOB 误差差值，误差增大越多，说明该特征对模型性能的贡献越大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 加载数据集\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 训练随机森林模型，启用 OOB 误差计算\n",
    "rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# 查看基于OOB的性能\n",
    "print(f\"OOB Score: {rf.oob_score_}\")  # rf.oob_score_ 表示模型在 OOB 样本上的基准性能。\n",
    "\n",
    "# 计算特征打乱后的 OOB 误差变化，观察模型性能的变化。\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def permutation_importance_oob(model, X_train, y_train):\n",
    "    \"\"\" 计算基于 OOB 误差的特征重要性。\n",
    "\n",
    "    parameter:\n",
    "        1. model   : 训练好的随机森林模型\n",
    "        2. X_train : 训练集特征\n",
    "        3. y_train : 训练集标签\n",
    "    \"\"\"\n",
    "    # 初始化变量\n",
    "    base_oob_score = model.oob_score_  # 基准OOB分数\n",
    "    feature_importances = np.zeros(X_train.shape[1])  # 保存特征重要性\n",
    "\n",
    "    # 遍历每个特征\n",
    "    for col in range(X_train.shape[1]):\n",
    "        X_train_permuted = X_train.copy()  # 创建训练集副本\n",
    "        np.random.shuffle(X_train_permuted[:, col])  # 随机打乱某个特征列\n",
    "\n",
    "        # 用打乱特征后的训练集重新计算OOB得分\n",
    "        model.fit(X_train_permuted, y_train)\n",
    "        oob_score_permuted = model.oob_score_\n",
    "\n",
    "        # 计算OOB误差的变化\n",
    "        feature_importances[col] = base_oob_score - oob_score_permuted  # 分数下降越多，特征越重要\n",
    "\n",
    "    return feature_importances\n",
    "\n",
    "# 调用函数计算特征重要性\n",
    "oob_importances = permutation_importance_oob(rf, X_train, y_train)\n",
    "\n",
    "# 输出结果\n",
    "for i, importance in enumerate(oob_importances):\n",
    "    print(f\"Feature {i} ({data.feature_names[i]}): OOB Importance {importance}\")\n",
    "\n",
    "# 可视化特征重要性\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"OOB Feature Importance\")\n",
    "plt.bar(range(X_train.shape[1]), oob_importances, align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), data.feature_names, rotation=45)\n",
    "plt.ylabel(\"OOB Score Decrease\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优点：\n",
    "\n",
    "- **模型无关**：虽然我们在这里使用的是随机森林模型，但这种OOB误差打乱方法可以应用于任何基于bootstrap的模型。\n",
    "\n",
    "- **直接评估**：相比于仅通过分裂节点的纯度变化来评估，基于OOB误差的方法能够直接反映特征对模型整体预测能力的影响。\n",
    "\n",
    "挑战：\n",
    "\n",
    "- **计算成本**：每次打乱特征后，都需要重新拟合模型并计算OOB分数。这对于大规模数据集或模型复杂度较高的情况，计算开销会较大。\n",
    "\n",
    "- **特征相关性问题**：如果两个特征高度相关，打乱其中一个特征后，另一个特征可能会“补偿”它的影响，这使得模型的性能下降幅度不明显，从而低估该特征的重要性。\n",
    "\n",
    "可以将OOB误差的重要性与**Gini不纯度减少**的特征重要性评分结合起来，综合评估模型中各特征的影响力。\n",
    "\n",
    "**基于 OOB 误差的特征重要性评估** 方法通过打乱某个特征并观察模型 OOB 得分的变化来衡量该特征的重要性。\n",
    "\n",
    "这种方法能直接反映特征对模型预测性能的影响，尤其适合随机森林模型。虽然计算复杂度较高，但它能提供更直观的特征评估方式，特别是在高维数据集中。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
